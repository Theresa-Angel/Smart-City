# -*- coding: utf-8 -*-
"""Smart City Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sMtkKu3wo-IH6ySec7A2D0SjoGIz28li
"""

# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nkoxFtSs6QTFQ7aeopnDLqPiIlltJ8Bc
"""

!pip install gradio pandas matplotlib torch
!pip install transformers PyPDF2

# -*- coding: utf-8 -*-
import gradio as gr
import pandas as pd
import matplotlib.pyplot as plt
import random
from datetime import datetime, timedelta
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import PyPDF2
import io

# Load model and tokenizer (from first file)
model_name = "ibm-granite/granite-3.3-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def generate_response(prompt, max_length=1024):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)

    if torch.cuda.is_available():
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.replace(prompt, "").strip()
    return response

def extract_text_from_pdf(pdf_file):
    if pdf_file is None:
        return ""

    try:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        return f"Error reading PDF: {str(e)}"

def eco_tips_generator(problem_keywords):
    prompt = f"Generate practical and actionable eco-friendly tips for sustainable living related to: {problem_keywords}. Provide specific solutions and suggestions:"
    return generate_response(prompt, max_length=1000)

def policy_summarization(pdf_file, policy_text):
    # Get text from PDF or direct input
    if pdf_file is not None:
        content = extract_text_from_pdf(pdf_file)
        summary_prompt = f"Summarize the following policy document and extract the most important points, key provisions, and implications:\n\n{content}"
    else:
        summary_prompt = f"Summarize the following policy document and extract the most important points, key provisions, and implications:\n\n{policy_text}"

    return generate_response(summary_prompt, max_length=1200)

# Functions from second file
def generate_city_health_data():
    dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(30, 0, -1)]

    data = {
        'Date': dates,
        'Air Quality': [random.randint(70, 100) for _ in range(30)],
        'Energy Consumption': [random.randint(200, 400) for _ in range(30)],
        'Waste Management': [random.randint(80, 100) for _ in range(30)],
        'Water Quality': [random.randint(85, 100) for _ in range(30)],
        'Green Coverage': [random.randint(75, 95) for _ in range(30)]
    }
    return pd.DataFrame(data)

def generate_eco_tips():
    tips = [
        "Use public transportation or bike instead of driving to reduce carbon emissions.",
        "Install solar panels on rooftops to harness renewable energy.",
        "Implement smart waste management systems with sensors for efficient collection.",
        "Create more green spaces and urban gardens to improve air quality.",
        "Use energy-efficient LED lighting in streets and buildings.",
        "Promote water conservation with smart irrigation systems.",
        "Encourage electric vehicle adoption with charging stations throughout the city.",
        "Implement green building standards for all new constructions.",
        "Develop urban farming initiatives to reduce food transportation needs.",
        "Use permeable pavements to reduce stormwater runoff and recharge groundwater."
    ]
    return random.sample(tips, 5)

# LLM response for chat assistant
def get_llm_response(message, history):
    prompt = f"As a Sustainable Smart City Assistant, respond to this query: {message}. Provide helpful, actionable advice about urban sustainability, green technology, or eco-friendly practices:"
    return generate_response(prompt, max_length=500)

# Feedback submission function
def submit_feedback(feedback_type, category, feedback_text):
    # In a real application, you would save the feedback to a database here
    # For now, we'll just return a success message and clear the inputs

    # Create HTML for the popup
    popup_html = """
    <div id="feedback-popup" style="
        position: fixed;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        background: white;
        padding: 20px;
        border-radius: 10px;
        box-shadow: 0 4px 20px rgba(0,0,0,0.2);
        z-index: 1000;
        text-align: center;
        border: 2px solid #4CAF50;
        min-width: 300px;
    ">
        <h3 style="color: #4CAF50; margin-top: 0;">Thank You!</h3>
        <p>Your feedback has been submitted successfully.</p>
        <button onclick="document.getElementById('feedback-popup').style.display='none'"
                style="
                    background: #4CAF50;
                    color: white;
                    border: none;
                    padding: 8px 16px;
                    border-radius: 4px;
                    cursor: pointer;
                    margin-top: 10px;
                ">OK</button>
    </div>
    """

    # Return the HTML popup and reset the feedback form
    return popup_html, None, None, ""

# Create the merged Gradio interface
with gr.Blocks(theme=gr.themes.Soft(primary_hue="green", secondary_hue="emerald")) as demo:
    gr.Markdown("#  Sustainable Smart City Assistant")

    # HTML element for the popup (initially empty)
    popup = gr.HTML("")

    with gr.Row():
        with gr.Column(scale=2):
            # City Health Dashboard

            # Eco Tips Generator (from first file)
            with gr.Tab("Eco Tips Generator"):
                gr.Markdown("## üí° Eco Tips Generator")
                with gr.Row():
                    with gr.Column():
                        keywords_input = gr.Textbox(
                            label="Environmental Problem/Keywords",
                            placeholder="e.g., plastic, solar, water waste, energy saving...",
                            lines=3
                        )
                        generate_tips_btn = gr.Button("Generate Eco Tips")

                    with gr.Column():
                        tips_output = gr.Textbox(label="Sustainable Living Tips", lines=15)

                generate_tips_btn.click(eco_tips_generator, inputs=keywords_input, outputs=tips_output)

            # Policy Summarization (from first file)
            with gr.Tab("Policy Analysis"):
                gr.Markdown("## üìë Policy Analysis")
                with gr.Row():
                    with gr.Column():
                        pdf_upload = gr.File(label="Upload Policy PDF", file_types=[".pdf"])
                        policy_text_input = gr.Textbox(
                            label="Or paste policy text here",
                            placeholder="Paste policy document text...",
                            lines=5
                        )
                        summarize_btn = gr.Button("Analyze Policy")

                    with gr.Column():
                        summary_output = gr.Textbox(label="Policy Summary & Key Points", lines=20)

                summarize_btn.click(policy_summarization, inputs=[pdf_upload, policy_text_input], outputs=summary_output)

            # Citizen Feedback (from second file)
            with gr.Tab("Citizen Feedback"):
                gr.Markdown("## üí¨ Provide Your Feedback")
                with gr.Row():
                    feedback_type = gr.Radio(["Suggestion", "Complaint", "Appreciation"], label="Feedback Type", value="Suggestion")
                    category = gr.Dropdown(["Environment", "Transportation", "Waste Management", "Energy", "Other"],
                                         label="Category", value="Environment")
                feedback_text = gr.Textbox(label="Your Feedback", lines=3)
                submit_btn = gr.Button("Submit Feedback")

                # Set up the feedback submission
                submit_btn.click(
                    submit_feedback,
                    inputs=[feedback_type, category, feedback_text],
                    outputs=[popup, feedback_type, category, feedback_text]
                )

        with gr.Column(scale=1):
            # Chat with Assistant (from second file with LLM integration)
            gr.Markdown("## üó®Ô∏è Chat with Assistant")
            chatbot = gr.Chatbot(label="Conversation", height=400)
            msg = gr.Textbox(label="Your Message", placeholder="Ask about sustainability or smart city initiatives...")
            clear = gr.Button("Clear")


    # Define interactions for chat
    def respond(message, chat_history):
        bot_message = get_llm_response(message, chat_history)
        chat_history.append((message, bot_message))
        return "", chat_history

    msg.submit(respond, [msg, chatbot], [msg, chatbot])
    clear.click(lambda: None, None, chatbot, queue=False)

if __name__ == "__main__":
    demo.launch()